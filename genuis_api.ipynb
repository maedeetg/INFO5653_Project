{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f8a401d-799d-4882-82a1-ea380231df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7ae734d-4b5b-455e-b5eb-2796704965fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = \"yxeLnXDdwoDJunEPDRUgYVmH-I703sGZKWGoCdKbFNupYKHugalkOVqtzh3aSe82\"\n",
    "client_secret = \"zd3utRAeYAprwImrgEPERa6BPjNrJMOcg4ogsbTAR-uCoIoh9zu28ufhMVRYlBTpb8kSHkWK75LDrHSerdEOag\"\n",
    "client_access_token = \"wVsdPF-_vbLLypQZ3qaR0EwytwJW2jKBsK-PoZwv6BDKQLDK0gkUwstOYuH-EDD2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01a8e157-809a-4650-b8b9-28a5b2a6159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_song_info(artist_name, song_title):\n",
    "    base_url = 'https://api.genius.com'\n",
    "    headers = {'Authorization': f'Bearer {client_access_token}'}\n",
    "    search_url = base_url + '/search'\n",
    "    params = {'q': f'{artist_name} {song_title}'}\n",
    "\n",
    "    response = requests.get(search_url, params = params, headers = headers)\n",
    "    return response\n",
    "\n",
    "def request_song_url(artist_name, song_titles):\n",
    "    # Input is song titles, want to find associated urls with each\n",
    "    urls = []\n",
    "    track_names = []\n",
    "\n",
    "    for song in song_titles:\n",
    "        #print(song)\n",
    "        song = re.sub(\"Â´\", \"'\", song)\n",
    "        song = re.sub(\"  \", \" \", song)\n",
    "        response = request_song_info(artist_name, song)\n",
    "        json = response.json()\n",
    "    \n",
    "        # Extract the first matching song URL\n",
    "        hit = json['response']['hits'][0]['result']\n",
    "        #print(hit)\n",
    "        #print(hit['title'].lower())\n",
    "        if (artist_name.lower() in hit['primary_artist']['name'].lower()):\n",
    "            song_url = hit['url']\n",
    "            urls.append(song_url)\n",
    "            \n",
    "            match = re.search(r'beatles-(.*?)-lyrics', song_url)\n",
    "    \n",
    "            if match:\n",
    "                result = match.group(1) \n",
    "                track_names.append(result)\n",
    "        \n",
    "    # Return both the URL and the song title\n",
    "    return [urls, track_names] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90f73abc-c541-4d8f-8514-fa83d79675a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First want to only have songs that are composed by lennon or mccartney\n",
    "os.chdir(\"C:/Users/maede/Downloads/Python/INFO5653/Project/Project/INFO5653_Project\")\n",
    "df = pd.read_csv(\"song_composer_singer.csv\")\n",
    "df.columns = [\"Song\", \"Composer\", \"Singer\"]\n",
    "lennon_mccartney = df[df['Composer'].isin([\"Lennon\", \"McCartney\"])]\n",
    "\n",
    "# Now want to match songs from this set to the lyrics pulled from genuis api\n",
    "l_m_songs = list(lennon_mccartney['Song'])\n",
    "\n",
    "# Composer list\n",
    "l_m_composer = list(lennon_mccartney['Composer'])\n",
    "\n",
    "# Find urls!\n",
    "artist_name = \"The Beatles\"\n",
    "[urls, song_titles] = request_song_url(artist_name, l_m_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d9e9680-0527-4d6f-9691-4719b3a5e74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(song_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16743177-e618-4f4a-8f0a-e227dda4b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape lyrics from a Genius.com song URL\n",
    "def scrape_song_lyrics(url):\n",
    "    song = requests.get(url)\n",
    "    html = BeautifulSoup(song.content, 'html.parser')\n",
    "    lyrics_divs = html.find_all(\"div\", {\"data-lyrics-container\": \"true\"})\n",
    "    \n",
    "    if lyrics_divs:\n",
    "        lyrics = \"\\n\".join(div.get_text(separator=\"\\n\") for div in lyrics_divs)\n",
    "        # Remove identifiers like chorus, verse, etc\n",
    "        lyrics = re.sub(r'[\\(\\[].*?[\\)\\]]', '', lyrics, flags=re.DOTALL).strip()\n",
    "        \n",
    "        # Remove empty lines\n",
    "        lyrics = os.linesep.join([s for s in lyrics.splitlines() if s])  \n",
    "    else:\n",
    "        print(\"Lyrics not found!\")\n",
    "        lyrics = \"\"\n",
    "    \n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "745c4d5c-c858-4ef7-b958-eab1a5a48b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_lyrics_to_corpus(urls, track_names, composer, path):\n",
    "    for i in range(len(track_names)):\n",
    "        # Get url and title\n",
    "        url = urls[i]\n",
    "        title = track_names[i]\n",
    "        label = composer[i]\n",
    "        title = re.sub(\" \", \"-\", title)\n",
    "        title = re.sub(r\"\\?\", \"\", title)\n",
    "        title = title.lower()\n",
    "    \n",
    "        # Create file, mypath, and filename\n",
    "        file = str(label) + '-' + str(title) + \".txt\"\n",
    "        filename = mypath + \"/\" + file\n",
    "    \n",
    "        # Open file to write to\n",
    "        my_file = open(filename, \"w\", encoding = \"utf-8\")\n",
    "    \n",
    "        # Find lyrics\n",
    "        lyrics = scrape_song_lyrics(url)\n",
    "        my_file.write(lyrics)\n",
    "        my_file.close()\n",
    "\n",
    "mypath = \"C:/Users/maede/Downloads/Python/INFO5653/Project/Project/INFO5653_Project/Composer_Lyrics_Corpus\"\n",
    "write_lyrics_to_corpus(urls, song_titles, l_m_composer, mypath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98477c2d-3473-4ddf-a495-83ad7cbb9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## PART 3: USE CORPUS TO BUILD DATAFRAME ##########\n",
    "\n",
    "# Path to corpus\n",
    "path = \"C:/Users/maede/Downloads/Python/INFO5653/Project/Project/INFO5653_Project/Composer_Lyrics_Corpus\"\n",
    "\n",
    "# Create file name\n",
    "file_name_list = os.listdir(path)[1:]\n",
    "\n",
    "# Initliaze empty lists that will be filled later with all file paths in corpus\n",
    "complete_file_paths = [] \n",
    "list_file_names = []\n",
    "    \n",
    "for name in file_name_list:\n",
    "    nextfile = path + \"/\" + name \n",
    "    complete_file_paths.append(nextfile) \n",
    "    nextnameL = name.split(\".\")\n",
    "    list_file_names.append(nextnameL[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fed959ee-1530-427a-87ae-9d170192259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE COUNTVECTORIZOR AND LET MAX_FEATURES = 100\n",
    "\n",
    "# Now we can use CountVectorizer \n",
    "my_vect = CountVectorizer(input = 'filename', stop_words = 'english', token_pattern=r'\\b[a-zA-Z]{4,}\\b', max_features = 100)\n",
    "fit = my_vect.fit_transform(complete_file_paths)\n",
    "column_names = my_vect.get_feature_names_out()\n",
    "\n",
    "# Convert to dataframe\n",
    "lyrics_df = pd.DataFrame(fit.toarray(),columns = column_names)\n",
    "\n",
    "# Add labels using a dictionary\n",
    "dict_labels = {}\n",
    "\n",
    "for i in range(0, len(complete_file_paths)):\n",
    "    dict_labels[i] = list_file_names[i].split(\"-\")[0]\n",
    "\n",
    "lyrics_df = lyrics_df.rename(dict_labels, axis = \"index\")\n",
    "lyrics_df.index.name = \"LABEL\"\n",
    "\n",
    "# Write dataframe to csv\n",
    "lyrics_df.to_csv(\"paul_john_lyrics_corpus_count100.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7efb84e-2f53-49f6-a63b-5e9b43b9a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE TfidfVectorizer AND LET MAX_FEATURES = 100\n",
    "# 100\n",
    "my_vect = TfidfVectorizer(input = 'filename', stop_words = 'english', token_pattern=r'\\b[a-zA-Z]{4,}\\b', max_features = 100)\n",
    "fit = my_vect.fit_transform(complete_file_paths)\n",
    "column_names = my_vect.get_feature_names_out()\n",
    "\n",
    "# Convert to dataframe\n",
    "lyrics_df = pd.DataFrame(fit.toarray(),columns = column_names)\n",
    "\n",
    "# Add labels using a dictionary\n",
    "dict_labels = {}\n",
    "\n",
    "for i in range(0, len(complete_file_paths)):\n",
    "    dict_labels[i] = list_file_names[i].split(\"-\")[0]\n",
    "\n",
    "lyrics_df = lyrics_df.rename(dict_labels, axis = \"index\")\n",
    "lyrics_df.index.name = \"LABEL\"\n",
    "\n",
    "# Write dataframe to csv\n",
    "lyrics_df.to_csv(\"paul_john_lyrics_corpus_tfid100.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0061c7ce-8891-498d-930a-a146c5cfff10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maede\\New folder\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LEMMING, CountVectorizer, max_features = 100\n",
    "LEMMER = WordNetLemmatizer() \n",
    "\n",
    "def MY_LEMMER(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z\\-]\", \" \", str_input).lower().split()\n",
    "    words = [LEMMER.lemmatize(word) for word in words]\n",
    "    return words\n",
    "    \n",
    "my_vect = CountVectorizer(input = 'filename', tokenizer = MY_LEMMER, max_features = 100)\n",
    "fit = my_vect.fit_transform(complete_file_paths)\n",
    "column_names = my_vect.get_feature_names_out()\n",
    "\n",
    "# Convert to dataframe\n",
    "lyrics_df = pd.DataFrame(fit.toarray(),columns = column_names)\n",
    "\n",
    "# Add labels using a dictionary\n",
    "dict_labels = {}\n",
    "\n",
    "for i in range(0, len(complete_file_paths)):\n",
    "    dict_labels[i] = list_file_names[i].split(\"-\")[0]\n",
    "\n",
    "lyrics_df = lyrics_df.rename(dict_labels, axis = \"index\")\n",
    "lyrics_df.index.name = \"Label\"\n",
    "\n",
    "# Write dataframe to csv\n",
    "lyrics_df.to_csv(\"paul_john_lyrics_corpus_lem_count100.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b88ad77e-9f70-4ad1-8b1c-cb2c3f9f32f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maede\\New folder\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LEMMING, TfidfVectorizer, max_features = 100\n",
    "LEMMER = WordNetLemmatizer() \n",
    "\n",
    "def MY_LEMMER(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z\\-]\", \" \", str_input).lower().split()\n",
    "    words = [LEMMER.lemmatize(word) for word in words]\n",
    "    return words\n",
    "    \n",
    "my_vect = TfidfVectorizer(input = 'filename', tokenizer = MY_LEMMER, max_features = 100)\n",
    "fit = my_vect.fit_transform(complete_file_paths)\n",
    "column_names = my_vect.get_feature_names_out()\n",
    "\n",
    "# Convert to dataframe\n",
    "lyrics_df = pd.DataFrame(fit.toarray(),columns = column_names)\n",
    "\n",
    "# Add labels using a dictionary\n",
    "dict_labels = {}\n",
    "\n",
    "for i in range(0, len(complete_file_paths)):\n",
    "    dict_labels[i] = list_file_names[i].split(\"-\")[0]\n",
    "\n",
    "lyrics_df = lyrics_df.rename(dict_labels, axis = \"index\")\n",
    "lyrics_df.index.name = \"Label\"\n",
    "\n",
    "# Write dataframe to csv\n",
    "lyrics_df.to_csv(\"paul_john_lyrics_corpus_lem_tfid100.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6ebb083-62f4-4cf3-927a-28a72b912ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maede\\New folder\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# STEMMING, CountVectorizer, max_features = 100\n",
    "STEMMER = PorterStemmer()\n",
    "def MY_STEMMER(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z\\-]\", \" \", str_input).lower().split()\n",
    "    words = [STEMMER.stem(word) for word in words]\n",
    "    return words\n",
    "    \n",
    "my_vect = CountVectorizer(input = 'filename', tokenizer = MY_STEMMER, max_features = 100)\n",
    "fit = my_vect.fit_transform(complete_file_paths)\n",
    "column_names = my_vect.get_feature_names_out()\n",
    "\n",
    "# Convert to dataframe\n",
    "lyrics_df = pd.DataFrame(fit.toarray(),columns = column_names)\n",
    "\n",
    "# Add labels using a dictionary\n",
    "dict_labels = {}\n",
    "\n",
    "for i in range(0, len(complete_file_paths)):\n",
    "    dict_labels[i] = list_file_names[i].split(\"-\")[0]\n",
    "\n",
    "lyrics_df = lyrics_df.rename(dict_labels, axis = \"index\")\n",
    "lyrics_df.index.name = \"Label\"\n",
    "\n",
    "# Write dataframe to csv\n",
    "lyrics_df.to_csv(\"paul_john_lyrics_corpus_stem_count100.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77ff6d7b-402e-4171-9583-058e001eaadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maede\\New folder\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# STEMMING, TfidfVectorizer, max_features = 100\n",
    "STEMMER = PorterStemmer()\n",
    "def MY_STEMMER(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z\\-]\", \" \", str_input).lower().split()\n",
    "    words = [STEMMER.stem(word) for word in words]\n",
    "    return words\n",
    "    \n",
    "my_vect = TfidfVectorizer(input = 'filename', tokenizer = MY_STEMMER, max_features = 100)\n",
    "fit = my_vect.fit_transform(complete_file_paths)\n",
    "column_names = my_vect.get_feature_names_out()\n",
    "\n",
    "# Convert to dataframe\n",
    "lyrics_df = pd.DataFrame(fit.toarray(),columns = column_names)\n",
    "\n",
    "# Add labels using a dictionary\n",
    "dict_labels = {}\n",
    "\n",
    "for i in range(0, len(complete_file_paths)):\n",
    "    dict_labels[i] = list_file_names[i].split(\"-\")[0]\n",
    "\n",
    "lyrics_df = lyrics_df.rename(dict_labels, axis = \"index\")\n",
    "lyrics_df.index.name = \"Label\"\n",
    "\n",
    "# Write dataframe to csv\n",
    "lyrics_df.to_csv(\"paul_john_lyrics_corpus_stem_tfid100.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa8b7b9-d652-402c-ba8e-1bd5e5933ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
