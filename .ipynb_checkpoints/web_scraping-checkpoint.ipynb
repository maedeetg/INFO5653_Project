{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "af98e8b7-f11c-4c1a-9cb7-d0884c60acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import wordcloud\n",
    "import re\n",
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "88dcbdc6-257a-4d26-85be-bc546fe343da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### First website that I am scraping. This website has all beatles songs, \n",
    "# who wrote them, and who sang them\n",
    "\n",
    "def get_rid(words, clean = None):\n",
    "    # Need this so we do not reset clean at each recursion\n",
    "    if clean is None:\n",
    "        clean = [] \n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if \"total\" in words[i].lower():\n",
    "            j = i\n",
    "            continue\n",
    "        if \"singer\" in words[i].lower():\n",
    "            k = i\n",
    "            keep1 = words[:j]\n",
    "            keep2 = words[i+1:]\n",
    "            clean.extend(keep1)\n",
    "\n",
    "            return get_rid(keep2, clean = clean)\n",
    "    return clean \n",
    "\n",
    "## Get the data\n",
    "url = \"https://www.myrsten.nu/worldnet/beatlesongs.htm\"\n",
    "res = requests.get(url)\n",
    "htmlData = res.content\n",
    "\n",
    "## Parse the Data\n",
    "soup = BeautifulSoup(htmlData, \"html.parser\")\n",
    "\n",
    "# Find all <td> elements\n",
    "td_elements = soup.find_all(\"td\")\n",
    "\n",
    "# We want to store song, composer, and singer. \n",
    "songs = []\n",
    "\n",
    "# Skip headers, then process title, composer, and singer\n",
    "td_texts = [td.get_text(strip=True) for td in td_elements[4:]]  # Skip headers\n",
    "#print(td_texts)\n",
    "\n",
    "# One part where the text is a different langauge, change that\n",
    "index1 = td_texts.index(\"Låt\")\n",
    "index2 = td_texts.index(\"Huvudkompositör\")\n",
    "index3 = td_texts.index(\"Sångare\")\n",
    "\n",
    "td_texts[index1] = \"Song\"\n",
    "td_texts[index2] = \"Main composer\"\n",
    "td_texts[index3] = \"Singer\"\n",
    "\n",
    "# There is some text that we want to remove before saving song_title, composer, and singer\n",
    "clean = get_rid(td_texts)\n",
    "\n",
    "for i in range(0, len(clean), 3):\n",
    "    if i + 2 < len(clean):\n",
    "        song_title = clean[i]\n",
    "        #print(song_title)\n",
    "        composer = clean[i + 1]\n",
    "        singer = clean[i + 2]\n",
    "\n",
    "        # Some cleaning\n",
    "        song_title = song_title.replace(\"\\n\", \" \")\n",
    "        composer = composer.replace(\"\\n\", \" \")\n",
    "        singer = singer.replace(\"\\n\", \" \")\n",
    "        song_title = re.sub(r'\\\"', ' ', song_title, flags = re.IGNORECASE)\n",
    "        composer = re.sub(r'\\\"', ' ', composer, flags = re.IGNORECASE)\n",
    "        singer = re.sub(r'\\\"', ' ', singer, flags = re.IGNORECASE)\n",
    "        song_title = re.sub(r',', ' ', song_title, flags = re.IGNORECASE)\n",
    "        composer = re.sub(r',', ' ', composer, flags = re.IGNORECASE)\n",
    "        singer = re.sub(r',', ' ', singer, flags = re.IGNORECASE)\n",
    "        song_title = re.sub(r'\\d+', '', song_title, flags = re.IGNORECASE)\n",
    "\n",
    "        song_title = song_title.lstrip()\n",
    "        song_title = song_title.rstrip() \n",
    "        song_title = song_title.strip()\n",
    "        composer = composer.lstrip()\n",
    "        composer = composer.rstrip() \n",
    "        composer = composer.strip()\n",
    "        singer = singer.lstrip()\n",
    "        singer = singer.rstrip() \n",
    "        singer = singer.strip()\n",
    "\n",
    "        # if song_title not in [\"\", \"\\\\\", '\"', \"'\", \"*\", \":\", \";\"]:\n",
    "        #     # Remove digits\n",
    "        #     if not re.search(r'\\d', song_title):\n",
    "        #         print('before', song_title)\n",
    "        #         song_title = re.search(r'\\d', song_title)\n",
    "        #         print('after', song_title)\n",
    "\n",
    "\n",
    "        # re.sub(r'\\d+', '', text)\n",
    "        songs.append([song_title, composer, singer])\n",
    "\n",
    "# Write to csv file\n",
    "filename = \"song_composer_singer.csv\"\n",
    "\n",
    "# Open file\n",
    "my_file = open(filename, \"w\", encoding='utf8')\n",
    "\n",
    "# Write first rows, which are column names\n",
    "write_this = \"Song, Composer, Singer \\n\"\n",
    "my_file.write(write_this)\n",
    "my_file.close()\n",
    "\n",
    "my_file = open(filename, \"a\", encoding='utf8')\n",
    "\n",
    "for group in songs:\n",
    "    song = group[0] \n",
    "    composer = group[1]\n",
    "    singer = group[2]\n",
    "\n",
    "    write_this = str(song) + \",\" + str(composer) + \",\" + str(singer) + \"\\n\"\n",
    "    my_file.write(write_this)\n",
    "\n",
    "my_file.close()\n",
    "\n",
    "# First want to only have songs that are composed by lennon or mccartney\n",
    "os.chdir(\"C:/Users/maede/Downloads/Python/INFO5653/Project/Project_Part1/INFO5653_Project\")\n",
    "df = pd.read_csv(\"song_composer_singer.csv\", index_col=None)\n",
    "df.columns = [\"Song\", \"Composer\", \"Singer\"]\n",
    "lennon_mccartney = df.loc[df['Composer'].isin([\"Lennon\", \"McCartney\"])]\n",
    "lennon_mccartney.to_csv(\"lennon_mccartney.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "170b75ca-f81b-4302-87ab-09d7b9ea72b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mccartney\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "lennon\n",
      "lennon\n",
      "lennon\n",
      "lennon\n",
      "lennon\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "mccartney\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "mccartney\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "lennon\n",
      "mccartney\n",
      "mccartney\n",
      "mccartney\n",
      "mccartney\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "mccartney\n",
      "mccartney\n",
      "mccartney\n",
      "mccartney\n",
      "mccartney\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "mccartney\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "mccartney\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "mccartney\n",
      "mccartney\n",
      "mccartney\n",
      "mccartney\n",
      "mccartney\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "lennon\n",
      "mccartney\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "mccartney\n",
      "mccartney\n",
      "mccartney\n",
      "mccartney\n",
      "lennon\n",
      "lennon\n",
      "lennon\n"
     ]
    }
   ],
   "source": [
    "# For this part, we are looking at song titles as our data.\n",
    "# Open path, create file, open file, write column names, close file\n",
    "path = \"C:/Users/maede/Downloads/Python/INFO5653/Project/Project_Part1/INFO5653_Project/lennon_mccartney.csv\"\n",
    "file = open(path,\"r\", encoding=\"utf-8\")\n",
    "filename = \"lennon_mccartney_clean.csv\"\n",
    "new_file = open(filename,\"w\", encoding=\"utf-8\")\n",
    "to_write = \"Label,Song Title\\n\"\n",
    "new_file.write(to_write)\n",
    "new_file.close()\n",
    "\n",
    "# Open file to append to\n",
    "new_file = open(filename, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "# Initliaze empty dataframe\n",
    "tennis_pickleball_clean_df = pd.DataFrame()\n",
    "output_file = \"lennon_mccartney.txt\"\n",
    "outfile = open(output_file,\"w\", encoding=\"utf-8\")\n",
    "outfile.close()\n",
    "outfile = open(output_file,\"a\", encoding=\"utf-8\") \n",
    "\n",
    "# Skip reading in the column names\n",
    "next(file) \n",
    "for row in file:\n",
    "    # Remove any necessary spaces from each row\n",
    "    row = re.sub(r'^\\d+,', '', row)\n",
    "    row = row.lstrip()\n",
    "    row = row.rstrip() \n",
    "    row = row.strip()\n",
    "    \n",
    "    # Split at each space to look at words\n",
    "    words = re.split(\" \", row)\n",
    "    words_clean = []\n",
    "\n",
    "    for word in words:\n",
    "        # Clean words!\n",
    "        word = word.lower() \n",
    "        word = word.lstrip()\n",
    "        # word = word.replace(\",\",\"\") \n",
    "        word = word.replace(\" \",\"\")\n",
    "        word = word.replace(\"_\",\"\" )\n",
    "        word = re.sub('\\+', ' ', word) \n",
    "        word = re.sub('.*\\+\\n', '', word)\n",
    "        word = re.sub('zz+', ' ', word)\n",
    "        word = word.replace(\"\\t\",\"\")\n",
    "        word = word.replace(\".\",\"\")\n",
    "        word = word.strip()\n",
    "        words_clean.append(word)\n",
    "    \n",
    "    #print(words_clean[-1])\n",
    "    last_word = words_clean[-1].split(',')[0]\n",
    "    label = words_clean[-1].split(',')[-2]\n",
    "    words_clean[-1] = last_word\n",
    "    \n",
    "    if \"lennon\" in label:\n",
    "        label = \"john\"\n",
    "    if \"mccartney\" in label:\n",
    "        label = \"paul\"\n",
    "   \n",
    "    #print(label)\n",
    "    \n",
    "    text = \" \".join(words_clean)\n",
    "    \n",
    "    ### More cleaning\n",
    "    text = text.replace(\"\\\\n\",\"\")\n",
    "    text = text.strip(\"\\\\n\")\n",
    "    text = text.replace(\"\\\\'\",\"\")\n",
    "    text = text.replace(\"\\\\\",\"\")\n",
    "    text = text.replace('\"',\"\")\n",
    "    text = text.replace(\"'\",\"\")\n",
    "    text = text.replace(\"s'\",\"\")\n",
    "    text = text.lstrip()\n",
    "    text = text.split(\",\")[-1]\n",
    "\n",
    "    # Write to files\n",
    "    write = label + \",\"+ text + \"\\n\"\n",
    "    new_file.write(write)\n",
    "    outfile.write(write)\n",
    "     \n",
    "# Close files\n",
    "file.close()  \n",
    "new_file.close()\n",
    "outfile.close()\n",
    "\n",
    "# Read in clean csv to use count vectorizor on\n",
    "clean = pd.read_csv(filename)\n",
    "\n",
    "# Some cleaning\n",
    "clean = clean.dropna(how = 'any', axis = 0)  ## axis 0 is rowwise\n",
    "\n",
    "# Store labels\n",
    "labels = clean[\"Label\"]\n",
    "\n",
    "# Store data without labels\n",
    "data = clean.drop([\"Label\"], axis=1) \n",
    "\n",
    "# Build a list of content that countvectorizer expects.\n",
    "my_list = [] \n",
    "\n",
    "for i in range(0, len(data)):\n",
    "    next_text = data.iloc[i,0]\n",
    "    my_list.append(next_text)\n",
    "\n",
    "# Now vectorize\n",
    "my_vect = CountVectorizer(input = 'content', max_features = 100)\n",
    "\n",
    "fit = my_vect.fit_transform(my_list)\n",
    "\n",
    "# Add columns names and labels\n",
    "column_names = my_vect.get_feature_names_out()\n",
    "lennon_mccartney_clean = pd.DataFrame(fit.toarray(), columns = column_names)\n",
    "lennon_mccartney_clean = lennon_mccartney_clean.rename(labels, axis = \"index\")\n",
    "lennon_mccartney_clean.index.name = \"LABEL\"\n",
    "\n",
    "# Write dataframe to csv\n",
    "lennon_mccartney_clean.to_csv(\"lennon_mccartney_clean.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0aa01b54-1c9f-4b6f-9f70-a772bcc1f637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Love Me Do',\n",
       " 'P.S. I Love You',\n",
       " 'Please Please Me',\n",
       " 'Ask Me Why',\n",
       " 'There´s a Place',\n",
       " 'I Saw Her Standing There',\n",
       " 'Do You Want to Know a Secret',\n",
       " 'Hold Me Tight',\n",
       " 'It Won´t Be Long',\n",
       " 'All My Loving',\n",
       " 'All I´ve Got to Do',\n",
       " 'Not a Second Time',\n",
       " 'I Call Your Name',\n",
       " 'Can´t Buy Me Love',\n",
       " 'You Can´t Do That',\n",
       " 'And I Love Her',\n",
       " 'I Should Have Known Better',\n",
       " 'Tell Me Why',\n",
       " 'If I fell',\n",
       " 'I´m Happy Just to Dance With You',\n",
       " 'A Hard Day´s Night',\n",
       " 'I´ll Cry Instead',\n",
       " 'I´ll Be Back',\n",
       " 'Any Time at All',\n",
       " 'Things We Said Today',\n",
       " 'When I Get Home',\n",
       " 'I´m a Loser',\n",
       " 'Every Little Thing',\n",
       " 'I Don´t Want to Spoil the Party',\n",
       " 'What You´re Doing',\n",
       " 'No Reply',\n",
       " 'Eight Days a Week',\n",
       " 'I´ll Follow the Sun',\n",
       " 'Ticket to Ride',\n",
       " 'Another Girl',\n",
       " 'The Night Before',\n",
       " 'You´ve Got to Hide Your Love Away',\n",
       " 'Tell Me What You See',\n",
       " 'You´re Going to Lose That Girl',\n",
       " 'Help!',\n",
       " 'I´ve Just Seen a Face',\n",
       " 'Yesterday',\n",
       " 'It´s Only Love',\n",
       " 'Run for Your Life',\n",
       " 'Norwegian Wood (This Bird Has Flown)',\n",
       " 'Drive My Car',\n",
       " 'Nowhere Man',\n",
       " 'I´m Looking Through You',\n",
       " 'Michelle',\n",
       " 'You Won´t See Me',\n",
       " 'Girl',\n",
       " 'Tomorrow Never Knows',\n",
       " 'Got to Get You Into My Life',\n",
       " 'Doctor Robert',\n",
       " 'And Your Bird Can Sing',\n",
       " 'I´m Only Sleeping',\n",
       " 'Eleanor Rigby',\n",
       " 'For No One',\n",
       " 'Yellow Submarine',\n",
       " 'Good Day Sunshine',\n",
       " 'Here  There and Everywhere',\n",
       " 'She Said She Said',\n",
       " 'When I´m 64',\n",
       " 'Sgt. Pepper´s Lonely Hearts Club Band',\n",
       " 'Good Morning Good Morning',\n",
       " 'Fixing a Hole',\n",
       " 'Being For the Benefit of Mr. Kite',\n",
       " 'Lovely Rita',\n",
       " 'Lucy in the Sky with Diamonds',\n",
       " 'Getting Better',\n",
       " 'She´s Leaving Home',\n",
       " 'With a Little Help From My Friends',\n",
       " 'Sgt. Pepper´s Lonely Hearts Club Band (Reprise)',\n",
       " 'Magical Mystery Tour',\n",
       " 'Your Mother Should Know',\n",
       " 'The Fool on the Hill',\n",
       " 'I Am the Walrus',\n",
       " 'Revolution 1',\n",
       " 'Revolution 9',\n",
       " 'Blackbird',\n",
       " 'Everybody´s Got Something to Hide Except Me and My Monkey',\n",
       " 'Good Night',\n",
       " 'Ob-la-di  Ob-la-da',\n",
       " 'Cry Baby Cry',\n",
       " 'Helter Skelter',\n",
       " 'Sexy Sadie',\n",
       " 'Mother Nature´s Son',\n",
       " 'Yer Blues',\n",
       " 'Rocky Raccoon',\n",
       " 'Wild Honey Pie',\n",
       " 'Back in the USSR',\n",
       " 'Dear Prudence',\n",
       " 'Glass Onion',\n",
       " 'I Will',\n",
       " 'Birthday',\n",
       " 'Happiness is a Warm Gun',\n",
       " 'Honey Pie',\n",
       " 'Martha My Dear',\n",
       " 'I´m So Tired',\n",
       " 'The Continuing Story of Bungalow Bill',\n",
       " 'Why Don´t We Do It in the Road?',\n",
       " 'Julia',\n",
       " 'Yellow Submarine',\n",
       " 'All Together Now',\n",
       " 'All You Need is Love',\n",
       " 'I Want You (She´s So Heavy)',\n",
       " 'Oh! Darling',\n",
       " 'You Never Give Me Your Money',\n",
       " 'Her Majesty',\n",
       " 'Golden Slumbers',\n",
       " 'Carry That Weight',\n",
       " 'Maxwell´s Silver Hammer',\n",
       " 'Come Together',\n",
       " 'The End',\n",
       " 'Sun King',\n",
       " 'Mean Mr Mustard',\n",
       " 'Polythene Pam',\n",
       " 'She Came in Through the Bathroom Window',\n",
       " 'Because',\n",
       " 'Across the Universe',\n",
       " 'Dig a Ponny',\n",
       " 'Get Back',\n",
       " 'Two of Us',\n",
       " 'Dig It',\n",
       " 'Let It Be',\n",
       " 'The Long and Winding Road',\n",
       " 'One After 909',\n",
       " 'This Boy  1963',\n",
       " 'She´s a Woman  1964',\n",
       " 'I Feel Fine  1964',\n",
       " 'Yes it is  1965',\n",
       " 'I´m Down  1965',\n",
       " 'Day Tripper  1965',\n",
       " 'Paperback Writer  1966',\n",
       " 'Rain  1966',\n",
       " 'Strawberry Fields Forever  1967',\n",
       " 'Penny Lane  1967',\n",
       " 'Hello  Goodbye  1967',\n",
       " 'Lady Madonna  1968',\n",
       " 'Hey Jude  1968',\n",
       " 'Don´t Let Me Down  1969',\n",
       " 'The Ballad of John and Yoko  1969',\n",
       " 'You Know My Name (Look up the Number)  1970']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_m_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12111192-0718-4ba6-ad36-778107e8cad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
