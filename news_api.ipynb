{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b39ac570-4a69-4bef-8919-e3310656b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import newspaper\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9df3d19-879e-4c74-83ff-8477b94fa868",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## BUILD CSV WITH LABELS, DESCRIPTION, AND CONTENT ##########\n",
    "\n",
    "api_key = \"4fe8b78ffdb64b0fb8dbfda6a3ba99f6\"\n",
    "url = \"https://newsapi.org/v2/everything\"\n",
    "\n",
    "url_paul = (\"https://newsapi.org/v2/everything?q=McCartney&domains=rollingstone.com&langauge=en&apiKey=4fe8b78ffdb64b0fb8dbfda6a3ba99f6\")\n",
    "url_john = (\"https://newsapi.org/v2/everything?q=Lennon&domains=rollingstone.com&langauge=en&apiKey=4fe8b78ffdb64b0fb8dbfda6a3ba99f6\")\n",
    "\n",
    "params_paul = {\"q\": \"Paul McCartney\",\n",
    "               \"language\": \"en\",\n",
    "               \"sortBy\": \"publishedAt\",\n",
    "               \"apiKey\": api_key}\n",
    "\n",
    "params_john = {\"q\": \"John Lennon\",\n",
    "               \"language\": \"en\",\n",
    "               \"sortBy\": \"publishedAt\",\n",
    "               \"apiKey\": api_key}\n",
    "\n",
    "response_paul = requests.get(url_paul)\n",
    "response_john = requests.get(url_john)\n",
    "\n",
    "if response_paul.status_code == 200:\n",
    "    json_txt_paul = response_paul.json()\n",
    "    paul_articles = json_txt_paul.get(\"articles\", [])\n",
    "    df_paul = pd.DataFrame(paul_articles)\n",
    "\n",
    "else:\n",
    "    print(f\"Error: {response_paul.status_code}, {response_paul.text}\")\n",
    "\n",
    "df_paul['full_text'] = df_paul['url'].apply(lambda url: (lambda a: (a.download(), a.parse(), a.text)[2])(Article(url)))\n",
    "df_paul['label'] = len(df_paul)*['paul']\n",
    "\n",
    "if response_john.status_code == 200:\n",
    "    json_txt_john = response_john.json()\n",
    "    john_articles = json_txt_john.get(\"articles\", [])\n",
    "    df_john = pd.DataFrame(john_articles)\n",
    "\n",
    "else:\n",
    "    print(f\"Error: {response_john.status_code}, {response_john.text}\")\n",
    "\n",
    "df_john['full_text'] = df_john['url'].apply(lambda url: (lambda a: (a.download(), a.parse(), a.text)[2])(Article(url)))\n",
    "df_john['label'] = len(df_john)*['john']\n",
    "\n",
    "df_paul_john = pd.concat([df_paul, df_john])\n",
    "df_paul_john_description = df_paul_john[['label', 'description']]\n",
    "df_paul_john_full_text = df_paul_john[['label', 'full_text']]\n",
    "df_paul_john_full_text.loc[:, 'full_text'] = df_paul_john_full_text['full_text'].replace(r'\\n', ' ', regex=True)\n",
    "\n",
    "df_paul_john_description.to_csv(\"paul_john_description.csv\", index = True)\n",
    "df_paul_john_full_text.to_csv(\"paul_john_full_text.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a82c7146-fb72-4174-bd54-14e2a18e601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## USE CSV TO BUILD DATAFRAME ##########\n",
    "# First, for descriptions\n",
    "\n",
    "# Path name\n",
    "path = \"C:/Users/maede/Downloads/Python/INFO5653/Project/Project_Part1/INFO5653_Project/paul_john_description.csv\"\n",
    "\n",
    "# Open path, create file, open file, write column names, close file\n",
    "file = open(path,\"r\", encoding=\"utf-8\")\n",
    "filename = \"paul_john_description_clean.csv\"\n",
    "new_file = open(filename,\"w\", encoding=\"utf-8\")\n",
    "to_write = \"Label,Description\\n\"\n",
    "new_file.write(to_write)\n",
    "new_file.close()\n",
    "\n",
    "# Open file to append to\n",
    "new_file = open(filename, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "# Initliaze empty dataframe\n",
    "paul_john_descrip_clean_df = pd.DataFrame()\n",
    "output_file = \"paul_john_description_clean.txt\"\n",
    "outfile = open(output_file,\"w\", encoding=\"utf-8\")\n",
    "outfile.close()\n",
    "outfile = open(output_file,\"a\", encoding=\"utf-8\") \n",
    "\n",
    "# Skip reading in the column names\n",
    "next(file) \n",
    "\n",
    "for row in file:\n",
    "    # Remove any necessary spaces from each row\n",
    "    row = row.lstrip()\n",
    "    row = row.rstrip() \n",
    "    row = row.strip()\n",
    "\n",
    "    # Split at each space to look at words\n",
    "    words = re.split(\" \", row)\n",
    "    # print(words)\n",
    "    words_clean = []\n",
    "\n",
    "    for word in words:\n",
    "        # Clean words!\n",
    "        word = word.lower()\n",
    "        word = word.lstrip()\n",
    "        # word = word.replace(\",\",\"\") \n",
    "        word = word.replace(\" \",\"\")\n",
    "        word = word.replace(\"_\",\"\" )\n",
    "        word = re.sub('\\+', ' ', word) \n",
    "        word = re.sub('.*\\+\\n', '', word)\n",
    "        word = re.sub('zz+', ' ', word)\n",
    "        word = word.replace(\"\\t\",\"\")\n",
    "        word = word.replace(\".\",\"\")\n",
    "        word = word.strip()\n",
    "        words_clean.append(word)\n",
    "\n",
    "    label = words_clean[0].split(\",\")[1]\n",
    "    first_word = words_clean[0].split(\",\")[2]\n",
    "    words_clean[0] = first_word\n",
    "    \n",
    "    if \"john\" in label:\n",
    "        label = \"john\"\n",
    "    else:\n",
    "        label = \"paul\"\n",
    "        \n",
    "    text = \" \".join(words_clean)\n",
    "    \n",
    "    ### More cleaning....\n",
    "    text = text.replace(\"\\\\n\",\"\")\n",
    "    text = text.strip(\"\\\\n\")\n",
    "    text = text.replace(\"\\\\'\",\"\")\n",
    "    text = text.replace(\"\\\\\",\"\")\n",
    "    text = text.replace('\"',\"\")\n",
    "    text = text.replace(\"'\",\"\")\n",
    "    text = text.replace(\",\",\"\")\n",
    "    text = text.replace(\"s'\",\"\")\n",
    "    text = text.lstrip()\n",
    "\n",
    "    # Write to files\n",
    "    write = label + \",\"+ text + \"\\n\"\n",
    "    new_file.write(write)\n",
    "    outfile.write(write)\n",
    "     \n",
    "# Close files\n",
    "file.close()  \n",
    "new_file.close()\n",
    "outfile.close()\n",
    "\n",
    "# Read in clean csv to use count vectorizor on\n",
    "clean = pd.read_csv(filename)\n",
    "\n",
    "# Some cleaning\n",
    "clean = clean.dropna(how = 'any', axis = 0)  ## axis 0 is rowwise\n",
    "\n",
    "# Store labels\n",
    "labels = clean[\"Label\"]\n",
    "\n",
    "# Store data without labels\n",
    "data = clean.drop([\"Label\"], axis=1) \n",
    "\n",
    "# Build a list of content that countvectorizer expects.\n",
    "my_list = [] \n",
    "\n",
    "for i in range(0, len(data)):\n",
    "    next_text = data.iloc[i,0]\n",
    "    my_list.append(next_text)\n",
    "\n",
    "# Now vectorize\n",
    "my_vect = CountVectorizer(input = 'content', stop_words = 'english', token_pattern=r'\\b[a-zA-Z]{4,}\\b', max_features = 50)\n",
    "\n",
    "fit = my_vect.fit_transform(my_list)\n",
    "\n",
    "# Add columns names and labels\n",
    "column_names = my_vect.get_feature_names_out()\n",
    "paul_john_descrip_clean_df = pd.DataFrame(fit.toarray(), columns = column_names)\n",
    "paul_john_descrip_clean_df = paul_john_descrip_clean_df.rename(labels, axis = \"index\")\n",
    "paul_john_descrip_clean_df.index.name = \"Label\"\n",
    "\n",
    "# Write dataframe to csv\n",
    "paul_john_descrip_clean_df.to_csv(\"paul_john_description_clean.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16bf2550-aa53-4ff4-baae-8a9ff5bedbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## USE CSV TO BUILD DATAFRAME ##########\n",
    "# Now for content\n",
    "\n",
    "# Path name\n",
    "path2 = \"C:/Users/maede/Downloads/Python/INFO5653/Project/Project_Part1/INFO5653_Project/paul_john_full_text.csv\"\n",
    "\n",
    "# Open path, create file, open file, write column names, close file\n",
    "file2 = open(path2,\"r\", encoding=\"utf-8\")\n",
    "filename2 = \"paul_john_full_text_clean.csv\"\n",
    "new_file2 = open(filename2,\"w\", encoding=\"utf-8\")\n",
    "to_write2 = \"Label,Content\\n\"\n",
    "new_file2.write(to_write2)\n",
    "new_file2.close()\n",
    "\n",
    "# Open file to append to\n",
    "new_file2 = open(filename2, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "# Initliaze empty dataframe\n",
    "paul_john_content_clean_df = pd.DataFrame()\n",
    "output_file2 = \"paul_john_full_text_clean.txt\"\n",
    "outfile2 = open(output_file2,\"w\", encoding=\"utf-8\")\n",
    "outfile2.close()\n",
    "outfile2 = open(output_file2,\"a\", encoding=\"utf-8\") \n",
    "\n",
    "# Skip reading in the column names\n",
    "next(file2) \n",
    "for row in file2:\n",
    "    # Remove any necessary spaces from each row\n",
    "    row = row.lstrip()\n",
    "    row = row.rstrip() \n",
    "    row = row.strip()\n",
    "    row = re.sub(r'\\n', \" \", row)\n",
    "\n",
    "    # Split at each space to look at words\n",
    "    words = re.split(\" \", row)\n",
    "    # print(words)\n",
    "    words_clean = []\n",
    "\n",
    "    for word in words:\n",
    "        # Clean words!\n",
    "        word = word.lower()\n",
    "        word = word.lstrip()\n",
    "        # word = word.replace(\",\",\"\") \n",
    "        word = word.replace(\" \",\"\")\n",
    "        word = word.replace(\"_\",\"\" )\n",
    "        word = re.sub('\\+', ' ', word) \n",
    "        word = re.sub('.*\\+\\n', '', word)\n",
    "        word = re.sub('zz+', ' ', word)\n",
    "        word = word.replace(\"\\t\",\"\")\n",
    "        word = word.replace(\".\",\"\")\n",
    "        word = word.strip()\n",
    "        words_clean.append(word)\n",
    "\n",
    "    label = words_clean[0].split(\",\")[1]\n",
    "    first_word = words_clean[0].split(\",\")[2]\n",
    "    words_clean[0] = first_word\n",
    "    \n",
    "    if \"john\" in label:\n",
    "        label = \"john\"\n",
    "    else:\n",
    "        label = \"paul\"\n",
    "        \n",
    "    text = \" \".join(words_clean)\n",
    "    \n",
    "    ### More cleaning....\n",
    "    text = text.replace(\"\\\\n\",\"\")\n",
    "    text = text.strip(\"\\\\n\")\n",
    "    text = text.replace(\"\\\\'\",\"\")\n",
    "    text = text.replace(\"\\\\\",\"\")\n",
    "    text = text.replace('\"',\"\")\n",
    "    text = text.replace(\"'\",\"\")\n",
    "    text = text.replace(\",\",\"\")\n",
    "    text = text.replace(\"s'\",\"\")\n",
    "    text = text.lstrip()\n",
    "\n",
    "    # Write to files\n",
    "    write = label + \",\"+ text + \"\\n\"\n",
    "    new_file2.write(write)\n",
    "    outfile2.write(write)\n",
    "     \n",
    "# Close files\n",
    "file2.close()  \n",
    "new_file2.close()\n",
    "outfile2.close()\n",
    "\n",
    "# Read in clean csv to use count vectorizor on\n",
    "clean2 = pd.read_csv(filename2)\n",
    "\n",
    "# Some cleaning\n",
    "clean2 = clean2.dropna(how = 'any', axis = 0)  ## axis 0 is rowwise\n",
    "\n",
    "# Store labels\n",
    "labels2 = clean2[\"Label\"]\n",
    "\n",
    "# Store data without labels\n",
    "data2 = clean2.drop([\"Label\"], axis=1) \n",
    "\n",
    "# Build a list of content that countvectorizer expects.\n",
    "my_list2 = [] \n",
    "\n",
    "for i in range(0, len(data2)):\n",
    "    next_text2 = data2.iloc[i,0]\n",
    "    my_list2.append(next_text2)\n",
    "\n",
    "# Now vectorize\n",
    "my_vect2 = CountVectorizer(input = 'content', stop_words = 'english', token_pattern=r'\\b[a-zA-Z]{4,}\\b', max_features = 100)\n",
    "\n",
    "fit2 = my_vect2.fit_transform(my_list2)\n",
    "\n",
    "# Add columns names and labels\n",
    "column_names2 = my_vect2.get_feature_names_out()\n",
    "paul_john_full_text_clean_df = pd.DataFrame(fit2.toarray(), columns = column_names2)\n",
    "paul_john_full_text_clean_df = paul_john_full_text_clean_df.rename(labels2, axis = \"index\")\n",
    "paul_john_full_text_clean_df.index.name = \"Label\"\n",
    "\n",
    "# Write dataframe to csv\n",
    "paul_john_full_text_clean_df.to_csv(\"paul_john_full_text_clean.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ca6de-b266-4003-bdfd-e79121ba47d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
