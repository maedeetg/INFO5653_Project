{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9698a79-c3a0-4589-b572-40f5c21c438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Feb  9 13:23:28 2025\n",
    "\n",
    "@author: profa\n",
    "\"\"\"\n",
    "\n",
    "########################################\n",
    "## Example Set 2 - Gates\n",
    "##\n",
    "## Topics: \n",
    "    # Data gathering via API\n",
    "    #  - URLs and GET\n",
    "    # Cleaning and preparing text DATA\n",
    "    # DTM and Data Frames\n",
    "    # Training and Testing at DT\n",
    "    # CLustering\n",
    "    ## LDA\n",
    "    \n",
    "#########################################    \n",
    "    \n",
    "    \n",
    "## ATTENTION READER...\n",
    "##\n",
    "## First, you will need to go to \n",
    "## https://newsapi.org/\n",
    "## https://newsapi.org/register\n",
    "## and get an API key\n",
    "\n",
    " \n",
    "\n",
    "################## DO NOT USE MY KEY!!\n",
    "## Get your own key. \n",
    "##\n",
    "###################################################\n",
    "\n",
    "\n",
    "### API KEY  - get a key!\n",
    "##https://newsapi.org/\n",
    "\n",
    "## Example URL\n",
    "## https://newsapi.org/v2/everything?\n",
    "## q=tesla&from=2021-05-20&sortBy=publishedAt&\n",
    "## apiKey=YOUR KEY HERE\n",
    "\n",
    "\n",
    "## What to import\n",
    "import requests  ## for getting data from a server\n",
    "import re   ## for regular expressions\n",
    "import pandas as pd    ## for dataframes and related\n",
    "from pandas import DataFrame\n",
    "\n",
    "## To tokenize and vectorize text type data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "## For word clouds\n",
    "## conda install -c conda-forge wordcloud\n",
    "## May also have to run conda update --all on cmd\n",
    "#import PIL\n",
    "#import Pillow\n",
    "#import wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random as rd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn import tree\n",
    "## conda install python-graphviz\n",
    "## restart kernel (click the little red x next to the Console)\n",
    "import graphviz\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "\n",
    "####################################\n",
    "##\n",
    "##  Step 1: Connect to the server\n",
    "##          Send a query\n",
    "##          Collect and clean the \n",
    "##          results\n",
    "####################################\n",
    "\n",
    "####################################################\n",
    "##In the following loop, we will query thenewsapi servers\n",
    "##for all the topic names in the list\n",
    "## We will then build a large csv file \n",
    "## where each article is a row\n",
    "##\n",
    "## From there, we will convert this data\n",
    "## into a labeled dataframe\n",
    "## so we can train and then test our DT\n",
    "## model\n",
    "####################################################\n",
    "\n",
    "####################################################\n",
    "## Build the URL and GET the results\n",
    "## NOTE: At the bottom of this code\n",
    "## commented out, you will find a second\n",
    "## method for doing the following. This is FYI.\n",
    "####################################################\n",
    "\n",
    "## This is the endpoint - the server and \n",
    "## location on the server where your data \n",
    "## will be retrieved from\n",
    "\n",
    "## TEST FIRST!\n",
    "## We are about to build this URL:\n",
    "## https://newsapi.org/v2/everything?apiKey=8f4134fYOUR Links to an external site.KEY HERE 91e22100f22b&q=bitcoin\n",
    "\n",
    " \n",
    "\n",
    "topics=[\"politics\", \"analytics\", \"business\", \"sports\"]\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## topics needs to be a list of strings (words)\n",
    "## Next, let's build the csv file\n",
    "## first and add the column names\n",
    "## Create a new csv file to save the headlines\n",
    "filename=\"NewHeadlines.csv\"\n",
    "MyFILE=open(filename,\"w\")\n",
    "### Place the column names in - write to the first row\n",
    "WriteThis=\"LABEL,Date,Source,Title,Headline\\n\"\n",
    "MyFILE.write(WriteThis)\n",
    "MyFILE.close()\n",
    "\n",
    "## CHeck it! Can you find this file?\n",
    "    \n",
    "#### --------------------> GATHER - CLEAN - CREATE FILE    \n",
    "\n",
    "## RE: documentation and options\n",
    "## https://newsapi.org/docs/endpoints/everything\n",
    "\n",
    "endpoint=\"https://newsapi.org/v2/everything\"\n",
    "\n",
    "################# enter for loop to collect\n",
    "################# data on three topics\n",
    "#######################################\n",
    "\n",
    "for topic in topics:\n",
    "\n",
    "    ## Dictionary Structure\n",
    "    URLPost = {'apiKey':'8f413YOUR KEY91e22100f22b',\n",
    "               'q':topic}\n",
    "\n",
    "    response=requests.get(endpoint, URLPost)\n",
    "    print(response)\n",
    "    jsontxt = response.json()\n",
    "    print(jsontxt)\n",
    "    #####################################################\n",
    "    \n",
    "    \n",
    "    ## Open the file for append\n",
    "    MyFILE=open(filename, \"a\")\n",
    "    LABEL=topic\n",
    "    for items in jsontxt[\"articles\"]:\n",
    "        print(items, \"\\n\\n\\n\")\n",
    "                  \n",
    "        #Author=items[\"author\"]\n",
    "        #Author=str(Author)\n",
    "        #Author=Author.replace(',', '')\n",
    "        \n",
    "        Source=items[\"source\"][\"id\"]\n",
    "        print(Source)\n",
    "        \n",
    "        Date=items[\"publishedAt\"]\n",
    "        ##clean up the date\n",
    "        NewDate=Date.split(\"T\")\n",
    "        Date=NewDate[0]\n",
    "        print(Date)\n",
    "        \n",
    "        ## CLEAN the Title\n",
    "        ##----------------------------------------------------------\n",
    "        ##Replace punctuation with space\n",
    "        # Accept one or more copies of punctuation         \n",
    "        # plus zero or more copies of a space\n",
    "        # and replace it with a single space\n",
    "        Title=items[\"title\"]\n",
    "        Title=str(Title)\n",
    "        #print(Title)\n",
    "        Title=re.sub(r'[,.;@#?!&$\\-\\']+', ' ', str(Title), flags=re.IGNORECASE)\n",
    "        Title=re.sub(' +', ' ', str(Title), flags=re.IGNORECASE)\n",
    "        Title=re.sub(r'\\\"', ' ', str(Title), flags=re.IGNORECASE)\n",
    "        print(Title)\n",
    "        # and replace it with a single space\n",
    "        ## NOTE: Using the \"^\" on the inside of the [] means\n",
    "        ## we want to look for any chars NOT a-z or A-Z and replace\n",
    "        ## them with blank. This removes chars that should not be there.\n",
    "        Title=re.sub(r'[^a-zA-Z]', \" \", str(Title), flags=re.VERBOSE)\n",
    "        Title=Title.replace(',', '')\n",
    "        Title=' '.join(Title.split())\n",
    "        Title=re.sub(\"\\n|\\r\", \"\", Title)\n",
    "        ##----------------------------------------------------------\n",
    "        \n",
    "        Headline=items[\"description\"]\n",
    "        Headline=str(Headline)\n",
    "        Headline=re.sub(r'[,.;@#?!&$\\-\\']+', ' ', Headline, flags=re.IGNORECASE)\n",
    "        Headline=re.sub(' +', ' ', Headline, flags=re.IGNORECASE)\n",
    "        Headline=re.sub(r'\\\"', ' ', Headline, flags=re.IGNORECASE)\n",
    "        Headline=re.sub(r'[^a-zA-Z]', \" \", Headline, flags=re.VERBOSE)\n",
    "        ## Be sure there are no commas in the headlines or it will\n",
    "        ## write poorly to a csv file....\n",
    "        Headline=Headline.replace(',', '')\n",
    "        Headline=' '.join(Headline.split())\n",
    "        Headline=re.sub(\"\\n|\\r\", \"\", Headline)\n",
    "        \n",
    "        ### AS AN OPTION - remove words of a given length............\n",
    "        Headline = ' '.join([wd for wd in Headline.split() if len(wd)>3])\n",
    "    \n",
    "        #print(\"Author: \", Author, \"\\n\")\n",
    "        #print(\"Title: \", Title, \"\\n\")\n",
    "        #print(\"Headline News Item: \", Headline, \"\\n\\n\")\n",
    "        \n",
    "        #print(Author)\n",
    "        print(Title)\n",
    "        print(Headline)\n",
    "        \n",
    "        WriteThis=str(LABEL)+\",\"+str(Date)+\",\"+str(Source)+\",\"+ str(Title) + \",\" + str(Headline) + \"\\n\"\n",
    "        \n",
    "        MyFILE.write(WriteThis)\n",
    "        \n",
    "    ## CLOSE THE FILE\n",
    "    MyFILE.close()\n",
    "    \n",
    "################## END for loop\n",
    "\n",
    "####################################################\n",
    "##\n",
    "## Where are we now?\n",
    "## \n",
    "## So far, we have created a csv file\n",
    "## with labeled data. Each row is a news article\n",
    "##\n",
    "## - BUT - \n",
    "## We are not done. We need to choose which\n",
    "## parts of this data to use to model our decision tree\n",
    "## and we need to convert the data into a data frame.\n",
    "##\n",
    "########################################################\n",
    "\n",
    "\n",
    "BBC_DF=pd.read_csv(filename)\n",
    "print(BBC_DF.head())\n",
    "# iterating the columns \n",
    "for col in BBC_DF.columns: \n",
    "    print(col) \n",
    "    \n",
    "print(BBC_DF[\"Headline\"])\n",
    "\n",
    "## REMOVE any rows with NaN in them\n",
    "BBC_DF = BBC_DF.dropna()\n",
    "print(BBC_DF[\"Headline\"])\n",
    "\n",
    "### Tokenize and Vectorize the Headlines\n",
    "## Create the list of headlines\n",
    "## Keep the labels!\n",
    "\n",
    "HeadlineLIST=[]\n",
    "LabelLIST=[]\n",
    "\n",
    "for nexthead, nextlabel in zip(BBC_DF[\"Headline\"], BBC_DF[\"LABEL\"]):\n",
    "    HeadlineLIST.append(nexthead)\n",
    "    LabelLIST.append(nextlabel)\n",
    "\n",
    "print(\"The headline list is:\\n\")\n",
    "print(HeadlineLIST)\n",
    "\n",
    "print(\"The label list is:\\n\")\n",
    "print(LabelLIST)\n",
    "\n",
    "\n",
    "##########################################\n",
    "## Remove all words that match the topics.\n",
    "## For example, if the topics are food and covid\n",
    "## remove these exact words.\n",
    "##\n",
    "## We will need to do this by hand. \n",
    "NewHeadlineLIST=[]\n",
    "\n",
    "for element in HeadlineLIST:\n",
    "    print(element)\n",
    "    print(type(element))\n",
    "    ## make into list\n",
    "    AllWords=element.split(\" \")\n",
    "    print(AllWords)\n",
    "    \n",
    "    ## Now remove words that are in your topics\n",
    "    NewWordsList=[]\n",
    "    for word in AllWords:\n",
    "        print(word)\n",
    "        word=word.lower()\n",
    "        if word in topics:\n",
    "            print(word)\n",
    "        else:\n",
    "            NewWordsList.append(word)\n",
    "            \n",
    "    ##turn back to string\n",
    "    NewWords=\" \".join(NewWordsList)\n",
    "    ## Place into NewHeadlineLIST\n",
    "    NewHeadlineLIST.append(NewWords)\n",
    "\n",
    "\n",
    "##\n",
    "## Set the     HeadlineLIST to the new one\n",
    "HeadlineLIST=NewHeadlineLIST\n",
    "print(HeadlineLIST)     \n",
    "#########################################\n",
    "##\n",
    "##  Build the labeled dataframe\n",
    "##\n",
    "######################################################\n",
    "\n",
    "### Vectorize\n",
    "## Instantiate your CV\n",
    "MyCountV=CountVectorizer(\n",
    "        input=\"content\",  ## because we have a csv file\n",
    "        lowercase=True, \n",
    "        stop_words = \"english\",\n",
    "        max_features=50\n",
    "        )\n",
    "\n",
    "## Use your CV \n",
    "MyDTM = MyCountV.fit_transform(HeadlineLIST)  # create a sparse matrix\n",
    "print(type(MyDTM))\n",
    "\n",
    "\n",
    "ColumnNames=MyCountV.get_feature_names_out()\n",
    "#print(type(ColumnNames))\n",
    "\n",
    "\n",
    "## Build the data frame\n",
    "MyDTM_DF=pd.DataFrame(MyDTM.toarray(),columns=ColumnNames)\n",
    "\n",
    "## Convert the labels from list to df\n",
    "Labels_DF = DataFrame(LabelLIST,columns=['LABEL'])\n",
    "\n",
    "## Check your new DF and you new Labels df:\n",
    "print(\"Labels\\n\")\n",
    "print(Labels_DF)\n",
    "print(\"News df\\n\")\n",
    "print(MyDTM_DF.iloc[:,0:6])\n",
    "\n",
    "##Save original DF - without the lables\n",
    "My_Orig_DF=MyDTM_DF\n",
    "print(My_Orig_DF)\n",
    "######################\n",
    "## AND - just to make sure our dataframe is fair\n",
    "## let's remove columns called:\n",
    "## food, bitcoin, and sports (as these are label names)\n",
    "######################\n",
    "#MyDTM_DF=MyDTM_DF.drop(topics, axis=1)\n",
    "\n",
    "\n",
    "## Now - let's create a complete and labeled\n",
    "## dataframe:\n",
    "dfs = [Labels_DF, MyDTM_DF]\n",
    "\n",
    "Final_News_DF_Labeled = pd.concat(dfs,axis=1, join='inner')\n",
    "## DF with labels\n",
    "print(Final_News_DF_Labeled)\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "#############################################\n",
    "##\n",
    "## Create Training and Testing Data\n",
    "##\n",
    "## Then model and test the Decision Tree\n",
    "##\n",
    "################################################\n",
    "\n",
    "\n",
    "## Before we start our modeling, let's visualize and\n",
    "## explore.\n",
    "\n",
    "##It might be very interesting to see the word clouds \n",
    "## for each  of the topics. \n",
    "##--------------------------------------------------------\n",
    "List_of_WC=[]\n",
    "\n",
    "for mytopic in topics:\n",
    "\n",
    "    tempdf = Final_News_DF_Labeled[Final_News_DF_Labeled['LABEL'] == mytopic]\n",
    "    print(tempdf)\n",
    "    \n",
    "    tempdf =tempdf.sum(axis=0,numeric_only=True)\n",
    "    #print(tempdf)\n",
    "    \n",
    "    #Make var name\n",
    "    NextVarName=str(\"wc\"+str(mytopic))\n",
    "    #print( NextVarName)\n",
    "    \n",
    "    ##In the same folder as this code, I have three images\n",
    "    ## They are called: food.jpg, bitcoin.jpg, and sports.jpg\n",
    "    #next_image=str(str(mytopic) + \".jpg\")\n",
    "    #print(next_image)\n",
    "    \n",
    "    ## https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html\n",
    "    \n",
    "    ###########\n",
    "    ## Create and store in a list the wordcloud OBJECTS\n",
    "    #########\n",
    "    NextVarName = WordCloud(width=1000, height=600, background_color=\"white\",\n",
    "                   min_word_length=4, #mask=next_image,\n",
    "                   max_words=200).generate_from_frequencies(tempdf)\n",
    "    \n",
    "    ## Here, this list holds all three wordclouds I am building\n",
    "    List_of_WC.append(NextVarName)\n",
    "    \n",
    "\n",
    "##------------------------------------------------------------------\n",
    "print(List_of_WC)\n",
    "##########\n",
    "########## Create the wordclouds\n",
    "##########\n",
    "fig=plt.figure(figsize=(25, 25))\n",
    "#figure, axes = plt.subplots(nrows=2, ncols=2)\n",
    "NumTopics=len(topics)\n",
    "for i in range(NumTopics):\n",
    "    print(i)\n",
    "    ax = fig.add_subplot(NumTopics,1,i+1)\n",
    "    plt.imshow(List_of_WC[i], interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(\"NewClouds.pdf\")\n",
    " \n",
    "    \n",
    " \n",
    "###########################################################\n",
    "##\n",
    "##\n",
    "##                  Clustering\n",
    "##\n",
    "##\n",
    "############################################################\n",
    "## Our DF\n",
    "print(My_Orig_DF)\n",
    "\n",
    "#from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "#from sklearn.cluster import KMeans\n",
    "\n",
    "My_KMean= KMeans(n_clusters=3)\n",
    "My_KMean.fit(My_Orig_DF)\n",
    "My_labels=My_KMean.predict(My_Orig_DF)\n",
    "print(My_labels)\n",
    "\n",
    "#from sklearn import preprocessing\n",
    "#from sklearn.cluster import KMeans\n",
    "#import seaborn as sns\n",
    "\n",
    "My_KMean2 = KMeans(n_clusters=4).fit(preprocessing.normalize(My_Orig_DF))\n",
    "My_KMean2.fit(My_Orig_DF)\n",
    "My_labels2=My_KMean2.predict(My_Orig_DF)\n",
    "print(My_labels2)\n",
    "\n",
    "My_KMean3= KMeans(n_clusters=3)\n",
    "My_KMean3.fit(My_Orig_DF)\n",
    "My_labels3=My_KMean3.predict(My_Orig_DF)\n",
    "print(\"Silhouette Score for k = 3 \\n\",silhouette_score(My_Orig_DF, My_labels3))\n",
    "\n",
    "\n",
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n",
    "#length of the document: called cosine similarity\n",
    "print(len(My_Orig_DF))\n",
    "cosdist = 1 - cosine_similarity(My_Orig_DF)\n",
    "print(cosdist)\n",
    "print(len(cosdist))\n",
    "print(np.round(cosdist,3))  #cos dist should be .02\n",
    "\n",
    "#----------------------------------------------------------\n",
    "## Hierarchical Clustering using ward and cosine sim\n",
    "labels=list(Final_News_DF_Labeled[\"LABEL\"])\n",
    "print(labels)\n",
    "print(len(labels))\n",
    "print(type(labels))\n",
    "linkage_matrix = ward(cosdist) #define the linkage_matrix \n",
    "#using ward clustering pre-computed distances\n",
    "print(linkage_matrix)\n",
    "print(len(linkage_matrix))\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "dn = dendrogram(linkage_matrix, labels=labels, leaf_rotation=90)\n",
    "plt.show()\n",
    "\n",
    " \n",
    "\n",
    "###############################################################\n",
    "##\n",
    "##               Model with two ML supervised options\n",
    "##\n",
    "##               DT\n",
    "##               NB (multinomial)\n",
    "##      \n",
    "###############################################################         \n",
    "## STEP 1   Create Training and Testing Data\n",
    "###############################################################\n",
    "## Write the dataframe to csv so you can use it later if you wish\n",
    "##\n",
    "Final_News_DF_Labeled.to_csv(\"Labeled_News_Data_from_API.csv\")\n",
    "TrainDF, TestDF = train_test_split(Final_News_DF_Labeled, test_size=0.3)\n",
    "print(TrainDF)\n",
    "print(TestDF)\n",
    "\n",
    "#################################################\n",
    "## STEP 2: Separate LABELS\n",
    "#################################################\n",
    "## IMPORTANT - YOU CANNOT LEAVE LABELS ON \n",
    "## Save labels\n",
    "\n",
    "### TEST ---------------------\n",
    "TestLabels=TestDF[\"LABEL\"]\n",
    "print(TestLabels)\n",
    "TestDF = TestDF.drop([\"LABEL\"], axis=1)\n",
    "print(TestDF)\n",
    "### TRAIN----------------------\n",
    "TrainLabels=TrainDF[\"LABEL\"]\n",
    "print(TrainLabels)\n",
    "## remove labels\n",
    "TrainDF = TrainDF.drop([\"LABEL\"], axis=1)\n",
    "\n",
    "##################################################\n",
    "## STEP 3:  Run MNB\n",
    "##################################################\n",
    "\n",
    "## Instantiate\n",
    "MyModelNB= MultinomialNB()\n",
    "\n",
    "## FIT\n",
    "MyNB=MyModelNB.fit(TrainDF, TrainLabels)\n",
    "#print(MyNB.classes_)\n",
    "#print(MyNB.class_count_)\n",
    "#print(MyNB.feature_log_prob_)\n",
    "\n",
    "\n",
    "Prediction = MyModelNB.predict(TestDF)\n",
    "print(np.round(MyModelNB.predict_proba(TestDF),2))\n",
    "\n",
    "## COnfusion Matrix Accuracies\n",
    "cnf_matrix = confusion_matrix(TestLabels, Prediction)\n",
    "print(\"\\nThe confusion matrix is:\")\n",
    "print(cnf_matrix)\n",
    "\n",
    "\n",
    "##################################################\n",
    "## STEP 3:  Run DT\n",
    "##################################################\n",
    "\n",
    "## Instantiate\n",
    "MyDT=DecisionTreeClassifier(criterion='entropy', ##\"entropy\" or \"gini\"\n",
    "                            splitter='best',  ## or \"random\" or \"best\"\n",
    "                            max_depth=None, \n",
    "                            min_samples_split=2, \n",
    "                            min_samples_leaf=1, \n",
    "                            min_weight_fraction_leaf=0.0, \n",
    "                            max_features=None, \n",
    "                            random_state=None, \n",
    "                            max_leaf_nodes=None, \n",
    "                            min_impurity_decrease=0.0, \n",
    "                            \n",
    "                            class_weight=None)\n",
    "\n",
    "##\n",
    "MyDT.fit(TrainDF, TrainLabels)\n",
    "print(ColumnNames)\n",
    "print(len(ColumnNames))\n",
    "print(Labels_DF)\n",
    "print(type(Labels_DF))\n",
    "## Convert to LIST\n",
    "Labels_DF_List = Labels_DF[\"LABEL\"].to_list()\n",
    "print(Labels_DF_List)\n",
    "print(type(Labels_DF_List))\n",
    "print(len(Labels_DF_List))\n",
    "\n",
    "plt.figure(figsize=(50,30))\n",
    "\n",
    "plot_tree(MyDT, feature_names=ColumnNames, \n",
    "          class_names=Labels_DF_List, \n",
    "          filled=True,\n",
    "          max_depth=5, \n",
    "          fontsize=10)\n",
    "       \n",
    "plt.show()\n",
    "\n",
    "# If saving the figure is needed:\n",
    "plt.savefig(\"decision_tree.jpeg\")\n",
    "\n",
    "## COnfusion Matrix\n",
    "print(\"Prediction\\n\")\n",
    "DT_pred=MyDT.predict(TestDF)\n",
    "print(DT_pred)\n",
    "    \n",
    "bn_matrix = confusion_matrix(TestLabels, DT_pred)\n",
    "print(\"\\nThe confusion matrix is:\")\n",
    "print(bn_matrix)\n",
    "\n",
    "feature_names=ColumnNames\n",
    "FeatureImp=MyDT.feature_importances_   \n",
    "indices = np.argsort(FeatureImp)[::-1]\n",
    "## print out the important features.....\n",
    "for f in range(TrainDF.shape[1]):\n",
    "    if FeatureImp[indices[f]] > 0:\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], FeatureImp[indices[f]]))\n",
    "        print (\"feature name: \", feature_names[indices[f]])\n",
    "        \n",
    "        \n",
    "        \n",
    "##############################################\n",
    "##\n",
    "##   LDA Topics Modeling\n",
    "##\n",
    "##\n",
    "#########################################################\n",
    "NUM_TOPICS=NumTopics\n",
    "lda_model = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10000, learning_method='online')\n",
    "#lda_model = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, learning_method='online')\n",
    "   \n",
    "lda_Z_DF = lda_model.fit_transform(My_Orig_DF)\n",
    "print(lda_Z_DF.shape)  # (NO_DOCUMENTS, NO_TOPICS)\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names_out()[i], topic[i])\n",
    "                    for i in topic.argsort()[:-top_n - 1:-1]])\n",
    " \n",
    "print(\"LDA Model:\")\n",
    "print_topics(lda_model, MyCountV)\n",
    "\n",
    "\n",
    "################ Another fun vis for LDA\n",
    "plt.figure(figsize=(50,30))\n",
    "word_topic = np.array(lda_model.components_)\n",
    "#print(word_topic)\n",
    "word_topic = word_topic.transpose()\n",
    "\n",
    "num_top_words = 15\n",
    "vocab_array = np.asarray(ColumnNames)\n",
    "\n",
    "#fontsize_base = 70 / np.max(word_topic) # font size for word with largest share in corpus\n",
    "fontsize_base = 40\n",
    "\n",
    "for t in range(NUM_TOPICS):\n",
    "    plt.subplot(1, NUM_TOPICS, t + 1)  # plot numbering starts with 1\n",
    "    plt.ylim(0, num_top_words + 0.5)  # stretch the y-axis to accommodate the words\n",
    "    plt.xticks([])  # remove x-axis markings ('ticks')\n",
    "    plt.yticks([]) # remove y-axis markings ('ticks')\n",
    "    plt.title('Topic #{}'.format(t))\n",
    "    top_words_idx = np.argsort(word_topic[:,t])[::-1]  # descending order\n",
    "    top_words_idx = top_words_idx[:num_top_words]\n",
    "    top_words = vocab_array[top_words_idx]\n",
    "    top_words_shares = word_topic[top_words_idx, t]\n",
    "    for i, (word, share) in enumerate(zip(top_words, top_words_shares)):\n",
    "        plt.text(0.3, num_top_words-i-0.5, word, fontsize=fontsize_base/2)\n",
    "                 ##fontsize_base*share)\n",
    "\n",
    "\n",
    "plt.savefig(\"TopicsVis.pdf\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b2135-5020-4534-a69f-27bf1b82515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####-----------------------------------------------------------\n",
    "#### PCA and Pairwise Correlation\n",
    "##   3D Scatterplot\n",
    "##   !! Dataset !!  (A sample of this data was used in this code)\n",
    "##   Link to Full Iris Dataset from Kaggle:\n",
    "##   https://www.kaggle.com/datasets/uciml/iris?resource=download\n",
    "##\n",
    "## Gates, 2024\n",
    "####------------------------------------------------------------\n",
    "##\n",
    "## Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import clear_output\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "##This is a small sample of the Iris dataset (link above)\n",
    "## !!! This is MY path :)  YOU need to update this to be YOUR path !!!\n",
    "\n",
    "path=\"C:/Users/profa/Desktop/UCB/Classes/ML CSCI 5612/Data/Iris_Subset2.csv\"\n",
    "DF=pd.read_csv(path)\n",
    "print(DF)\n",
    "OriginalDF=DF.copy()\n",
    "print(OriginalDF)\n",
    "##--------------------------------\n",
    "## Remove and save the label\n",
    "## Next, update the label so that \n",
    "## rather than names like \"Iris-setosa\"\n",
    "## we use numbers instead. \n",
    "## This will be necessary when we \"color\"\n",
    "## the data in our plot\n",
    "##---------------------------------------\n",
    "DFLabel=DF[\"Species\"]  ## Save the Label \n",
    "print(DFLabel)  ## print the labels\n",
    "print(type(DFLabel))  ## check the datatype you have\n",
    "DFLabel_string=DFLabel\n",
    "## Remap the label names from strongs to numbers\n",
    "MyDic={\"Iris-setosa\":0, \"Iris-versicolor\":1, \"Iris-virginica\":2}\n",
    "DFLabel = DFLabel.map(MyDic)  ## Update the label to your number remap values\n",
    "print(DFLabel) ## Print the labels to confirm \n",
    "## Now, remove the label from the original dataframe\n",
    "DF=DF.drop([\"Species\"], axis=1)\n",
    "print(DF) #Print the dataframe to confirm \n",
    "\n",
    "###-------------------------------------------\n",
    "### Standardize your dataset\n",
    "###-------------------------------------------\n",
    "scaler = StandardScaler() ##Instantiate\n",
    "DF=scaler.fit_transform(DF) ## Scale data\n",
    "print(DF)\n",
    "print(type(DF))\n",
    "print(DF.shape)\n",
    "\n",
    "###############################################\n",
    "###--------------PERFORM PCA------------------\n",
    "###############################################\n",
    "## Instantiate PCA and choose how many components\n",
    "MyPCA=PCA(n_components=3)\n",
    "# Project the original data into the PCA space\n",
    "Result=MyPCA.fit_transform(DF)\n",
    "## Print the values of the first component \n",
    "#print(Result[:,0]) \n",
    "print(Result) ## Print the new (transformed) dataset\n",
    "print(\"The eigenvalues:\", MyPCA.explained_variance_)\n",
    "## Proof\n",
    "MyCov=np.cov(Result.T)\n",
    "print(\"Covar of the PC PCA Matrix: \\n\", MyCov) ## The variance here (on the diagonal) will match the eigenvalues\n",
    "print(\"The relative eigenvalues are:\",MyPCA.explained_variance_ratio_)\n",
    "print(\"The actual eigenvalues are:\", MyPCA.explained_variance_)\n",
    "EVects=MyPCA.components_\n",
    "print(\"The eigenvectors are:\\n\",EVects)\n",
    "print(type(EVects))\n",
    "print(\"The shape of the eigenvector matrix is\\n\", EVects.shape)\n",
    "print(DF.shape)\n",
    "print(DF)\n",
    "## Proof to transform origial data to eigenbasis\n",
    "## using the eigenvectors matrix.\n",
    "print(EVects.T)\n",
    "##  (15, 4) @ (4, 3)\n",
    "Transf=DF@EVects.T\n",
    "print(\"Proof that the transformed data is the EVects @ Data\\n\",Transf)\n",
    "\n",
    "\n",
    "################################################\n",
    "# Extract loadings\n",
    "## Rerun PCA with all columns - no dim reduction\n",
    "############################################################\n",
    "MyPCA2=PCA(n_components=4)\n",
    "# Project the original data into the PCA space\n",
    "Result2=MyPCA2.fit_transform(DF)\n",
    "print(MyPCA2.components_) \n",
    "print(MyPCA2.components_.T) ## Makes the eigenvectors columnwise\n",
    "print(MyPCA2.explained_variance_ratio_) \n",
    "\n",
    "print(OriginalDF)\n",
    "print(len(OriginalDF))\n",
    "print(OriginalDF.columns[0:4])\n",
    "\n",
    "for i in range(1, len(OriginalDF[0:4].columns)):\n",
    "               print(i)\n",
    "               \n",
    "loadings = pd.DataFrame(MyPCA2.components_.T, \n",
    "                        columns=[f'PC{i}' for i in range(1, len(OriginalDF[0:4].columns))], \n",
    "                        index=OriginalDF.columns[0:4])\n",
    "print(loadings)\n",
    "\n",
    "## Print the most important variables using a threshold\n",
    "threshold = 0.4\n",
    "# Find features with loadings above the threshold for each principal component\n",
    "important_features = {}\n",
    "for column in loadings.columns:\n",
    "    important_features[column] = loadings.index[loadings[column].abs() > threshold].tolist()\n",
    "\n",
    "# Now 'important_features' dictionary contains the important features for each PC\n",
    "for pc, features in important_features.items():\n",
    "    print(f\"{pc}: {', '.join(features)}\")\n",
    "\n",
    "# Plot heatmap of loadings\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(loadings, annot=True, cmap='coolwarm')\n",
    "plt.title('Feature Importance in Principal Components')\n",
    "plt.show()\n",
    "#################################################\n",
    "## Visualize the transformed 3D dataset\n",
    "## we just created using PCA\n",
    "#################################################\n",
    "\n",
    "fig2 = plt.figure()\n",
    "    #figsize=(12, 12))\n",
    "ax2 = fig2.add_subplot(projection='3d')\n",
    "#Axes3D(fig2, rect=[0, 0, .90, 1], elev=48, azim=134)\n",
    "\n",
    "x=Result[:,0]\n",
    "y=Result[:,1] \n",
    "z=Result[:,2]\n",
    "print(y)\n",
    "\n",
    "ax2.scatter(x,y,z, cmap=\"RdYlGn\", edgecolor='k', s=200, c=DFLabel)\n",
    "#surf2 = ax2.plot_surface(x, y, z, cmap='viridis')\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('Y')\n",
    "ax2.set_zlabel('Z')\n",
    "ax2.set_title('3D PCA')\n",
    "#\n",
    "plt.show()\n",
    "plt.savefig(\"MyImage.jpeg\")\n",
    "\n",
    "############################################\n",
    "## Create Plot to Show Eigenvalues\n",
    "############################################\n",
    "fig3=plt.figure()\n",
    "ACCUM_eigenvalues = np.cumsum(MyPCA.explained_variance_)\n",
    "print(ACCUM_eigenvalues)\n",
    "print(MyPCA.explained_variance_ratio_)\n",
    "plt.bar(range(0,len(MyPCA.explained_variance_ratio_)), MyPCA.explained_variance_ratio_, \n",
    "        alpha=0.5, align='center', label='Individual Explained Variances')\n",
    "#plt.step(range(0,len(ACCUM_eigenvalues)), ACCUM_eigenvalues, where='mid',label='Cumulative Explained Variances')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.title(\"Eigenvalues: Percentage of Variance/Information\")\n",
    "#plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "###############################################\n",
    "## Create a DF of the most important features\n",
    "##################################################\n",
    "shape= MyPCA.components_.shape[0]\n",
    "#print(shape)\n",
    "feature_names=[\"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\", \"SepalLengthCm\"]\n",
    "\n",
    "most_important = [np.abs(MyPCA.components_[i]).argmax() for i in range(shape)]\n",
    "most_important_names = [feature_names[most_important[i]] for i in range(shape)]\n",
    "\n",
    "# Build a doctionary of the imprtant features by PC\n",
    "MyDic = {'PC{}'.format(i): most_important_names[i] for i in range(shape)}\n",
    "\n",
    "# build the dataframe\n",
    "Important_DF = pd.DataFrame(MyDic.items())\n",
    "print(Important_DF)\n",
    "\n",
    "###############################################\n",
    "## Create a biplot of the most important features\n",
    "##################################################\n",
    "PCA_dataset= Result ## recall that we ran PCA with 3 to get this above\n",
    "EVectors_as_columns=EVects.T\n",
    "print(EVectors_as_columns)\n",
    "## PCA  dataset\n",
    "print(PCA_dataset)\n",
    "\n",
    " \n",
    "\n",
    "def biplot(PCA_dataset, EVectors_as_columns, labels=feature_names):\n",
    "    xs = PCA_dataset[:, 0]\n",
    "    ys = PCA_dataset[:, 1]\n",
    "    n = EVectors_as_columns.shape[0]\n",
    "   \n",
    "    scalex = 1.0 / (xs.max() - xs.min())\n",
    "    scaley = 1.0 / (ys.max() - ys.min())\n",
    "    \n",
    "    plt.scatter(xs*scalex, ys*scaley, cmap=\"RdYlGn\", edgecolor='k', s=100, c=DFLabel)\n",
    "              \n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, EVectors_as_columns[i, 0], EVectors_as_columns[i, 1], color='r', alpha=0.5)\n",
    "        if labels is None:\n",
    "            plt.text(EVectors_as_columns[i, 0] * 1.07 , EVectors_as_columns[i, 1] * 1.07, \n",
    "                     \"Var\" + str(i + 1), color='g', ha='center', va='center')\n",
    "        else:\n",
    "            plt.text(EVectors_as_columns[i, 0] * 1.07, \n",
    "                     EVectors_as_columns[i, 1] * 1.07, labels[i], color='g', \n",
    "                     ha='center', va='center')\n",
    "\n",
    "# Plot biplot\n",
    "plt.figure(figsize=(10, 20))\n",
    "#figsize=(10, 16)\n",
    "biplot(PCA_dataset, EVectors_as_columns, labels=feature_names)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Biplot of Principal Components')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
